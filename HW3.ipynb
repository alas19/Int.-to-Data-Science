{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2214eb3b",
   "metadata": {},
   "source": [
    "# HOMEWORK 3\n",
    "LEYLA ALAS 090190739"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee0b83",
   "metadata": {},
   "source": [
    "With this homework, we are going to learn how to analyse a text from html format of news. The needed libraries and the functions are in the cell below. *requests* is for reading html url's. *nltk* is for analysing the text word and sentence level and it is used to evaluate polarity of each sentences from a text. *regex* was used for finding where sentences are ended. This homework we are using *spacy*'s NER. and lastly *en_core_web_sm* is for English vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "75925fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from bs4 import BeautifulSoup\n",
    "from xmltodict import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f827b4",
   "metadata": {},
   "source": [
    "+ The cell below contain the functions that I used the most during this homework.\n",
    "+ **getSubjectGuardian** take the subject and return it as dictionary format and we use **getText** with the returned dictionary's link element.\n",
    "+ I used the **unique** function to make the list elements unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ddf17f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectGuardian(subject):\n",
    "    with requests.get(f'https://www.theguardian.com/{subject}/rss') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "\n",
    "fashion = getSubjectGuardian('fashion')\n",
    "\n",
    "def getText(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "    return ' '.join([x.text for x in raw.find_all('p')])\n",
    "\n",
    "def unique(lisst):\n",
    "    unique_list = []\n",
    "    for x in lisst:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4c80b",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d278cd4",
   "metadata": {},
   "source": [
    "I used FOX's RSS feed for this question. Here is example for the links;\n",
    "+ National: https://moxie.foxnews.com/feedburner/national.xml\n",
    "+ World: https://moxie.foxnews.com/feedburner/world.xml\n",
    "+ Sports: https://moxie.foxnews.com/feedburner/sports.xml\n",
    "\n",
    "Then, I turned html url's to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dcce55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectFOX(subject):\n",
    "    with requests.get(f'https://moxie.foxnews.com/feedburner/{subject}.xml') as link:\n",
    "        raw = parse(link.text)\n",
    "    return raw['rss']['channel']['item']\n",
    "\n",
    "def getText1(url):\n",
    "    with requests.get(url) as link:\n",
    "        raw = BeautifulSoup(link.content,'html.parser')\n",
    "    return ' '.join([x.text for x in raw.find_all('p')])\n",
    "\n",
    "latest = getSubjectFOX('latest')\n",
    "textfox = getText(latest[3]['link'])\n",
    "textguardian = getText(fashion[2]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc8fc0",
   "metadata": {},
   "source": [
    "Here is the functions for question 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66b17b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(text,k):\n",
    "    sentences = processText(text)\n",
    "    matrix = getMatrix(sentences)\n",
    "    projection = PCA(n_components=1)\n",
    "    weights = projection.fit_transform(matrix.toarray())\n",
    "    res = list(zip(weights.transpose()[0],range(112),sentences))\n",
    "    tmp = sorted(res,key=lambda x: x[0],reverse=True)[:k]\n",
    "    return sorted(tmp, key=lambda x: x[1])\n",
    "\n",
    "def processText(text):\n",
    "    sentences = re.split(r'[\\.\\?\\!]',text)\n",
    "    return [re.sub(r'[^\\w\\s]','',x.lower()) for x in sentences]\n",
    "\n",
    "def getMatrix(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    return vectorizer.fit_transform(sentences)\n",
    "\n",
    "def getKeywords(text,sw,k):\n",
    "    sentences = processText(text)\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=sw)\n",
    "    matrix = vectorizer.fit_transform(sentences)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    \n",
    "    projection = PCA(n_components=1)\n",
    "    tmp = projection.fit_transform(matrix.transpose().toarray())\n",
    "    weights = tmp.transpose()[0]\n",
    "    \n",
    "    return sorted(zip(weights,words),key=lambda x: x[0], reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2f534",
   "metadata": {},
   "source": [
    "I introduced the stopwords in English to the system, and I removed the k-highest from the text and printed them on the screen.\n",
    "+ The first one is from Guardian and the second one is from FOX.\n",
    "+ I used fashion's the second news for Guardian and latest's the third new for FOX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1c7a147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.2519912366433217, 'bag'),\n",
       " (1.4258822827493554, 'baguette'),\n",
       " (1.0338630724556481, 'designed'),\n",
       " (0.9446542513943269, 'fendi'),\n",
       " (0.7092838113582342, 'name'),\n",
       " (0.7032832122194975, 'arm'),\n",
       " (0.7032832122194973, 'borrellipersson'),\n",
       " (0.7032832122194973, 'borrows'),\n",
       " (0.7032832122194973, 'crusty'),\n",
       " (0.7032832122194973, 'easily'),\n",
       " (0.7032832122194973, 'french'),\n",
       " (0.7032832122194973, 'laird'),\n",
       " (0.7032832122194973, 'loaf'),\n",
       " (0.7032832122194973, 'long'),\n",
       " (0.7032832122194973, 'nestles')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swEN = stopwords.words('english')\n",
    "getKeywords(textguardian,swEN,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98b6da9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wrapuette', ',', 'inspired', 'Baguette', 'bag', '2000s', ',', 'taps', 'logo', 'mania', '‘', 'irony', '’', '-loving', 'fashion', 'crowd', 'The', 'American', 'fast', 'food', 'chain', 'KFC', 'launched', '£198', 'insulated', 'leather', 'handbag', 'British', 'market', ',', 'designed', 'able', 'hold', 'one', 'chicken', 'chain', '’', 'Twister', 'Wraps', '.', 'The', '“', 'Wrapuette', '”', ',', 'inspired', 'Baguette', 'bag', '2000s', 'popularised', 'fashion', 'brands', 'Fendi', ',', 'Dior', 'Gucci', ',', 'sold', 'limited', 'edition', 'via', 'KFC', '’', 'online', 'shop', '.', 'According', 'website', ',', 'proceeds', 'go', 'KFC', 'Foundation', ',', 'says', ',', '“', 'supports', 'grassroots', 'non-profit', 'organisations', 'empower', 'young', 'people', 'across', 'UK', ',', 'helping', 'fulfil', 'potential', 'build', 'positive', 'future', '”', '.', 'The', 'handbag', 'comes', 'pebbled', 'textured', 'leather', 'KFC', '’', 'signature', 'tomato', 'red', ',', 'features', 'monochrome', 'image', 'Colonel', 'Sanders', 'Y2K-style', 'set', 'letter', 'charms', 'spelling', 'KFC', '.', 'It', 'lined', 'insulating', 'layer', 'keep', 'food', 'warm', '.', 'KFC', 'says', 'running', 'waiting', 'list', 'bag', '.', 'A', 'spokesperson', 'KFC', 'would', 'reveal', 'quantity', 'produced', ',', 'saying', '“', 'numbers', 'extremely', 'exclusive', '”', '.', 'They', 'added', 'bag', '“', 'made', 'Italian', 'leather', 'handcrafted', 'Savile', 'Row', '”', '.', '“', 'I', 'love', ',', '”', 'said', 'fashion', 'stylist', 'Natalie', 'Hartley', ',', 'added', 'name', 'waiting', 'list', '.', '“', 'It', '’', 'clever', '.', 'No', 'one', 'would', 'expect', 'KFC', 'bring', 'collectible', 'handbag', '.', '“', 'They', '’', 'hit', 'nail', 'head', 'going', 'Y2K', 'trend', '.', 'The', 'Baguette', 'bag', 'iconic.', '”', 'Hartley', 'notes', 'bag', '’', 'shape', '“', 'bit', 'street', '”', 'original', 'Baguette', '.', 'The', 'Baguette', 'bag', ',', 'originally', 'designed', 'Silvia', 'Venturini', 'Fendi', '1997', ',', 'so-called', ',', 'wrote', 'Vogue', '’', 'Laird', 'Borrelli-Persson', '2015', 'retrospective', 'It', 'bag', ':', '“', 'This', 'petite', 'shoulder', 'bag', 'nestles', 'arm', 'easily', 'long', ',', 'crusty', 'French', 'loaf', 'borrows', 'name.', '”', 'Hartley', ',', 'runs', 'vintage', 'shop', ',', 'Chillie', 'London', ',', 'already', 'owns', 'KFC', 'bucket', 'hat', ',', 'hoping', 'add', 'Wrapuette', 'collection', '.', 'Although', '’', 'sell', 'immediately', ',', 'envisages', 'strong', 'return', '.', 'Hartley', 'see', 'fashion', 'crowd', 'buying', '(', '“', 'They', '’', 'find', 'ironic', '”', ')', ',', 'well', '“', 'celebrity', 'kids', '-', 'unless', '’', 'vegan', '”', '.', '“', 'It', '’', 'tapping', 'social-media', 'savvy', 'Gen', 'Z', 'cohort', ',', '”', 'says', 'Jane', 'Collins', ',', 'senior', 'editor', 'footwear', 'accessories', 'trend-forecasting', 'agency', 'WGSN', '.', '“', 'It', 'plays', 'logo', 'mania', 'key', 'era', ',', 'also', 'high-low', 'trend', 'high', 'price', 'point', '.', 'It', '’', 'going', 'eBay', 'minutes', '.', '”']\n"
     ]
    }
   ],
   "source": [
    "text_tokens = word_tokenize(textguardian)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in swEN]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a9d91614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.6646603555354593, 'true'),\n",
       " (1.9578256243517402, 'use'),\n",
       " (1.724337222384731, 'call'),\n",
       " (1.724337222384731, 'false'),\n",
       " (1.2811617852066457, 'disinformation'),\n",
       " (1.0253070870116425, 'concept'),\n",
       " (0.9388946356145317, 'jankowicz'),\n",
       " (0.9388946356145317, 'nina'),\n",
       " (0.801960820387931, 'reject'),\n",
       " (0.7840140892340034, 'abused'),\n",
       " (0.7840140892340027, 'figure'),\n",
       " (0.7840140892340027, 'hunters'),\n",
       " (0.7840140892340027, 'inquiry'),\n",
       " (0.7840140892340027, 'know'),\n",
       " (0.7840140892340027, 'laptop')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getKeywords(textfox,swEN,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb08d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'material', 'may', 'published', ',', 'broadcast', ',', 'rewritten', ',', 'redistributed', '.', '©2022', 'FOX', 'News', 'Network', ',', 'LLC', '.', 'All', 'rights', 'reserved', '.', 'Quotes', 'displayed', 'real-time', 'delayed', 'least', '15', 'minutes', '.', 'Market', 'data', 'provided', 'Factset', '.', 'Powered', 'implemented', 'FactSet', 'Digital', 'Solutions', '.', 'Legal', 'Statement', '.', 'Mutual', 'Fund', 'ETF', 'data', 'provided', 'Refinitiv', 'Lipper', '.', 'Compact', 'Magazine', 'founder', 'Sohrab', 'Ahmari', 'unpacks', 'Biden', 'administration', \"'s\", 'Disinformation', 'Governance', 'Board', 'consequences', 'nation', '.', 'Compact', 'Magazine', 'founder', 'Sohrad', 'Ahmari', 'said', '``', 'Fox', '&', 'Friends', \"''\", 'Friday', 'Biden', 'administration', 'putting', 'forth', '``', 'politicized', \"''\", '``', 'dangerous', \"''\", 'policy', 'toward', '``', 'disinformation', '.', \"''\", 'Ahmari', 'said', 'DHS', \"'\", 'Disinformation', 'Governance', 'Board', 'new', 'director', ',', 'Nina', 'Jankowicz', ',', 'want', 'use', 'label', 'anything', 'runs', 'counter', 'liberal', 'narratives', 'administration', \"'s\", 'agenda', '.', 'MAYORKAS', 'TESTIFIES', 'DHS', 'IS', 'CREATING', '‘', 'DISINFORMATION', 'GOVERNANCE', 'BOARD', '’', 'SOHRAB', 'AHMARI', ':', 'This', 'concept', 'mis', 'disinformation', 'politicized', 'ridiculous', 'concept', ',', 'became', 'anxiety', 'liberal', 'elites', 'country', 'Europe', 'around', '2015', ',', '2016', ',', 'President', 'Trump', 'elected', 'Brexit', 'happened', '.', 'And', 'use', 'time', 'suggest', 'anything', ',', 'set', 'facts', 'arguments', 'run', 'whatever', 'liberals', 'pushing', 'moment', 'disinformation', '.', 'We', 'reject', 'term', '.', 'We', 'use', 'true', 'false', ',', 'use', 'inquiry', 'figure', 'whether', 'something', 'true', 'false', 'reject', 'concept', 'misinformation', 'see', \"'s\", 'abused', 'someone', 'like', 'Nina', 'Jankowicz', 'call', 'know', 'perfectly', 'true', 'reporting', 'Hunter', \"'s\", 'laptop', 'call', 'disinformation', '.', 'You', 'question', 'origins', 'COVID', '19', '?', 'Disinformation', '.', 'Oh', ',', 'six', 'months', 'later', ',', 'turns', 'right', '.', 'You', \"n't\", 'think', 'Black', 'Lives', 'Matter', 'riots', 'okay', '?', 'That', \"'s\", 'disinformation', '.', 'So', 'dangerous', ',', 'I', 'mean', ',', 'use', 'word', 'Orwellian', 'much', ',', 'legitimately', 'Orwellian', 'Ministry', 'Truth', 'stuff', '.', 'WATCH', 'FULL', 'INTERVIEW', 'BELOW', ':', 'Get', 'stories', 'need-to-know', 'powerful', 'name', 'news', 'delivered', 'first', 'thing', 'every', 'morning', 'inbox', 'Subscribed', 'You', \"'ve\", 'successfully', 'subscribed', 'newsletter', '!', 'This', 'material', 'may', 'published', ',', 'broadcast', ',', 'rewritten', ',', 'redistributed', '.', '©2022', 'FOX', 'News', 'Network', ',', 'LLC', '.', 'All', 'rights', 'reserved', '.', 'Quotes', 'displayed', 'real-time', 'delayed', 'least', '15', 'minutes', '.', 'Market', 'data', 'provided', 'Factset', '.', 'Powered', 'implemented', 'FactSet', 'Digital', 'Solutions', '.', 'Legal', 'Statement', '.', 'Mutual', 'Fund', 'ETF', 'data', 'provided', 'Refinitiv', 'Lipper', '.']\n"
     ]
    }
   ],
   "source": [
    "text_tokens = word_tokenize(textfox)\n",
    "tokens_without_sw = [word for word in text_tokens if not word in swEN]\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb093a5",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b089f7e",
   "metadata": {},
   "source": [
    "This function takes the url of the text as input. Then it loads the English vocabulary and makes the words in the text unique. Then, I group these words inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e053cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2func(url):\n",
    "    text = getText(url)\n",
    "        \n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "    ress = NER(text)\n",
    "    lisst = [(word.text, word.label_) for word in ress.ents]\n",
    "    b = unique(lisst)\n",
    "\n",
    "    proper = []\n",
    "    country = []\n",
    "    corporation = []\n",
    "    \n",
    "    for x in range(len(b)):\n",
    "\n",
    "        if b[x][1] == 'PERSON' or b[x][1] == 'LOC' or b[x][1] == 'WORK_OF_ART':\n",
    "            proper.append(b[x][0])\n",
    "        elif b[x][1] == 'GPE':\n",
    "            country.append(b[x][0])\n",
    "        elif b[x][1] == 'ORG' or b[x][1] == 'PRODUCT':\n",
    "            corporation.append(b[x][0])\n",
    "    new = {'Proper Names' : proper, 'Country Names' : country, 'Corporation Names' : corporation}\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0102ff73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Proper Names': ['Baguette',\n",
       "  'Wrapuette',\n",
       "  'Gucci',\n",
       "  'Savile Row',\n",
       "  'Natalie Hartley',\n",
       "  'Silvia Venturini Fendi',\n",
       "  'Laird Borrelli-Persson',\n",
       "  'Chillie London',\n",
       "  'Wrapuette',\n",
       "  'Gen',\n",
       "  'Jane Collins'],\n",
       " 'Country Names': ['UK', 'Hartley', 'Baguette'],\n",
       " 'Corporation Names': ['Wrapuette',\n",
       "  'Fendi',\n",
       "  'the KFC Foundation',\n",
       "  'Vogue',\n",
       "  'Hartley',\n",
       "  'eBay']}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2func(fashion[2]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829655",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3d60e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = getText(fashion[0]['link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd887175",
   "metadata": {},
   "source": [
    "With q3func I first found the most negative and most positive scores in the text. Then, I found which sentences are equal to these points in the list format, where we translated the text, with the search function. Finally, sentences return with their scores from q3func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cfab6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(values, polarity, tip):\n",
    "    for i in range(len(values)):\n",
    "        if values[i][1][tip] == polarity:\n",
    "            return i\n",
    "    return i\n",
    "\n",
    "def q3func(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentences = sent_tokenize(text)\n",
    "    values = [(x,analyzer.polarity_scores(x)) for x in sentences]\n",
    "    \n",
    "    polarity = ['','']\n",
    "    sentence = ['','']\n",
    "    \n",
    "    pos_polarity = [values[x][1]['pos'] for x in range(int(len(values)))]\n",
    "    polarity[0] = max(pos_polarity)\n",
    "    x = search(values, polarity[0], 'pos')\n",
    "    sentence[0] = values[x]\n",
    "    \n",
    "    neg_polarity = [values[x][1]['neg'] for x in range(int(len(values)))]\n",
    "    polarity[1] = max(neg_polarity)\n",
    "    x = search(values, polarity[1],'neg')\n",
    "    sentence[1] = values[x]\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "sentences = q3func(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4fecc4e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('“It’s so clever.',\n",
       "  {'neg': 0.0, 'neu': 0.378, 'pos': 0.622, 'compound': 0.5095}),\n",
       " ('No one would expect KFC to bring out a collectible handbag.',\n",
       "  {'neg': 0.196, 'neu': 0.804, 'pos': 0.0, 'compound': -0.296})]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
